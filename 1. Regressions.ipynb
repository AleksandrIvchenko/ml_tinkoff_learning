{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Демонстрационные примеры для задач линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RANSACRegressor\n",
    "from ipywidgets import interact\n",
    "from ipywidgets.widgets import FloatSlider, IntSlider\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986f388740e14ba8843e5184b71618d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='a', max=2.0, min=-2.0), FloatSlider(value=0.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(a=FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          b=FloatSlider(min=-1, max=1, step=0.1, value=0),\n",
    "          e=FloatSlider(min=0, max=2, step=0.01, value=0.1),\n",
    "          n=IntSlider(min=2, max=200, step=2, value=50))\n",
    "def linear_regression_1d(a, b, e=0.5, n=100):\n",
    "    \"\"\"\n",
    "    Оцениваем по формуле параметры a и b одномерной линейной регрессии: y = a*x + b\n",
    "    e : уровень шума\n",
    "    n : количество точек\n",
    "    \"\"\"\n",
    "    xmin = 0\n",
    "    xmax = 5\n",
    "    np.random.seed(42)\n",
    "    x = np.random.rand(n)*(xmax - xmin) + xmin\n",
    "    y = a*x + b + e*np.random.randn(n)\n",
    "    a_hat = ((x - x.mean())*(y - y.mean())).sum()/((x - x.mean())**2).sum()\n",
    "    # a_hat = (x*(y - y.mean())).sum()/(x*(x - x.mean())).sum()    \n",
    "    b_hat = y.mean() - a_hat*x.mean()\n",
    "    y_pred = a_hat*x + b_hat\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot([x.min(), x.max()], [x.min()*a + b, x.max()*a + b], label='true', c='b')\n",
    "    plt.plot([x.min(), x.max()], [x.min()*a_hat + b_hat, x.max()*a_hat + b_hat], label='pred', c='r')\n",
    "    plt.ylim(-12, 12)\n",
    "    plt.legend()\n",
    "    plt.title(f'a\\u0302={a_hat:.5} b\\u0302={b_hat:.5} r2={r2:.5} mse={mse:.5} mae={mae:.5}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0e5d9c5d1449178d1502ae846b604f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='a', max=2.0, min=-2.0), FloatSlider(value=1.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "@interact(a=FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          b=FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          c=FloatSlider(min=-1, max=1, step=0.1, value=0),\n",
    "          e=FloatSlider(min=0, max=2, step=0.01, value=0.1),\n",
    "          n=IntSlider(min=3, max=200, step=3, value=50))\n",
    "def linear_regression_2d(a, b, c, e=0.5, n=100):\n",
    "    \"\"\"\n",
    "    Оцениваем параметры a, b и с двумерной линейной регрессии: y = a*x1 + b*x2 + c\n",
    "    e : уровень шума\n",
    "    n : количество точек\n",
    "    \"\"\"\n",
    "    xmin = 0\n",
    "    xmax = 5\n",
    "    np.random.seed(42)\n",
    "    x0 = np.ones(n)\n",
    "    x1 = np.random.rand(n)*(xmax - xmin) + xmin\n",
    "    x2 = np.random.rand(n)*(xmax - xmin) + xmin\n",
    "    y = a*x1 + b*x2 + c + e*np.random.randn(n)\n",
    "    xt = np.stack((x0, x1, x2))\n",
    "    x = xt.transpose()\n",
    "    w_hat = np.linalg.inv(xt@x)@xt@y\n",
    "    c_hat, a_hat, b_hat = w_hat.tolist()\n",
    "    print(w_hat)\n",
    "    y_pred = a_hat*x1 + b_hat*x2 + c_hat\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x1, x2, y)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')\n",
    "    plt.ylim(-12, 12)\n",
    "    plt.title(f'a\\u0302={a_hat:.4} b\\u0302={b_hat:.4} c\\u0302={c_hat:.4} ' + \n",
    "              f'r2={r2:.4} mse={mse:.4} mae={mae:.4}')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d307ac4b216e411a9d928836c9b91ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='a', max=2.0, min=-2.0), FloatSlider(value=1.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(a=FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          b=FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          c=FloatSlider(min=-1, max=1, step=0.1, value=0),\n",
    "          k=FloatSlider(min=0, max=0.5, step=0.01, value=0.5),\n",
    "          e=FloatSlider(min=0, max=2, step=0.01, value=0.1),\n",
    "          n=IntSlider(min=3, max=200, step=3, value=50))\n",
    "def linear_regression_2d_multicollinearity(a, b, c, k, e=0.5, n=100):\n",
    "    \"\"\"\n",
    "    Оцениваем параметры a, b и с двумерной линейной регрессии: y = a*x1 + b*x2 + c\n",
    "    Моделирование полной или частичной мультиколлинеарности (k=0 для полной).\n",
    "    Используем различные способы оценки.\n",
    "    e : уровень шума\n",
    "    n : количество точек\n",
    "    \"\"\"\n",
    "    xmin = 0\n",
    "    xmax = 5\n",
    "    np.random.seed(42)\n",
    "    x0 = np.ones(n)\n",
    "    x1 = np.random.rand(n)*(xmax - xmin) + xmin\n",
    "    x2 = x1 + k*np.random.randn(n)\n",
    "    y = a*x1 + b*x2 + c + e*np.random.randn(n)\n",
    "    \n",
    "    # вычислим коэффициенты регрессии по формуле (XT@X)^-1@XT@Y\n",
    "    xt = np.stack((x0, x1, x2))\n",
    "    x = xt.transpose()\n",
    "    w_hat = np.zeros(3)\n",
    "    try:\n",
    "        w_hat = np.linalg.inv(xt@x)@xt@y\n",
    "    except BaseException as ex:\n",
    "        print(f'Raw formula error: {ex}')\n",
    "    c_hat, a_hat, b_hat = w_hat.tolist()\n",
    "    print(f'w_hat={w_hat} Raw formula')\n",
    "    \n",
    "    # вычислим псевдообратную матрицу с помощью SVD:\n",
    "    u, s, vh = np.linalg.svd(x)\n",
    "    s_inv = np.identity(3)*(s**-1)\n",
    "    x_pi = vh.transpose()@s_inv@u.transpose()[:3,:]\n",
    "    \n",
    "    # вычислим коэффициенты регрессии с помощью псевдообратной матрицы:\n",
    "    w_hat_pi = x_pi@y\n",
    "    print(f'w_hat={w_hat_pi} Pseudoinverse matrix')\n",
    "    \n",
    "    # вычислим коэффиценты регрессии с помощью LinearRegression\n",
    "    linear_regression = LinearRegression()\n",
    "    linear_regression.fit(x, y)\n",
    "    w_hat_lr = np.hstack((np.array(linear_regression.intercept_), linear_regression.coef_[1:]))\n",
    "    print(f'w_hat={w_hat_lr} LinearRegression')\n",
    "    \n",
    "    # вычислим коэффиценты регрессии с помощью Ridge\n",
    "    ridge = Ridge()\n",
    "    ridge.fit(x, y)\n",
    "    w_hat_ridge = np.hstack((np.array(ridge.intercept_), ridge.coef_[1:]))\n",
    "    print(f'w_hat={w_hat_ridge} Ridge')\n",
    "    \n",
    "    # вычислим коэффиценты регрессии с помощью LASSO\n",
    "    lasso = Lasso()\n",
    "    lasso.fit(x, y)\n",
    "    w_hat_lasso = np.hstack((np.array(lasso.intercept_), lasso.coef_[1:]))\n",
    "    print(f'w_hat={w_hat_lasso} LASSO')\n",
    "    \n",
    "    # выведем сингулярные числа ():\n",
    "    print(f'\\u03bb**0.5={s}')\n",
    "    # выведем собственные значения матрицы XT@X:\n",
    "    print(f'\\u03bb(XT@X)={s**2}  {round((s**2).max()/(s**2).min(),3)}')\n",
    "    \n",
    "    y_pred = a_hat*x1 + b_hat*x2 + c_hat\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    plt.scatter(x1, x2)\n",
    "    plt.xlabel('x1')\n",
    "    plt.xlim(-1, 6)\n",
    "    plt.ylabel('x2')\n",
    "    plt.ylim(-1, 6)\n",
    "    plt.title(f'a\\u0302={a_hat:.5} b\\u0302={b_hat:.5} c\\u0302={c_hat:.4} ' + \n",
    "              f'r2={r2:.5} mse={mse:.5} mae={mae:.5}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b822ec20814903ae0833b9e6298719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-2.0, description='a', max=2.0, min=-2.0), FloatSlider(value=-1.0, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(a=FloatSlider(min=-2, max=2, step=0.1, value=-2),\n",
    "          b=FloatSlider(min=-1, max=1, step=0.1, value=-1),\n",
    "          e=FloatSlider(min=0, max=2, step=0.01, value=0.1),\n",
    "          n=IntSlider(min=2, max=200, step=2, value=30),\n",
    "          ox=FloatSlider(min=0, max=5, step=0.1, value=2.5),\n",
    "          oy=FloatSlider(min=-2, max=2, step=0.1, value=0.5),\n",
    "          on=IntSlider(min=0, max=20, step=1, value=5))\n",
    "def linear_regression_1d_ransac(a, b, e=0.5, n=100, ox=0.5, oy=0.5, on=5):\n",
    "    \"\"\"\n",
    "    Оцениваем с помощью RANSAC параметры a и b одномерной линейной регрессии: y = a*x + b\n",
    "    e : уровень шума\n",
    "    n : количество точек\n",
    "    ox, oy : координаты центра выборосов\n",
    "    on : количество точек с выбросом\n",
    "    \"\"\"\n",
    "    xmin = 0\n",
    "    xmax = 5\n",
    "    np.random.seed(42)\n",
    "    x = np.random.rand(n)*(xmax - xmin) + xmin\n",
    "    y = a*x + b + e*np.random.randn(n)\n",
    "    xo = np.random.randn(on) + ox\n",
    "    yo = np.random.randn(on) + oy\n",
    "    X = np.hstack((x, xo)).reshape((n + on, 1))\n",
    "    Y = np.hstack((y, yo))\n",
    "    ransac_regressor = RANSACRegressor()\n",
    "    ransac_regressor.fit(X,Y)\n",
    "    a_hat = ransac_regressor.estimator_.coef_[0]\n",
    "    b_hat = ransac_regressor.estimator_.intercept_\n",
    "    \n",
    "    linear_regression = LinearRegression()\n",
    "    linear_regression.fit(X,Y)\n",
    "    lr_a_hat = linear_regression.coef_[0]\n",
    "    lr_b_hat = linear_regression.intercept_\n",
    "    \n",
    "    y_pred = a_hat*x + b_hat\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    plt.scatter(x, y, c='b', marker='o')\n",
    "    plt.scatter(xo, yo, c='r', marker='^')\n",
    "    plt.plot([x.min(), x.max()], [x.min()*a + b, x.max()*a + b], label='true', c='b')\n",
    "    plt.plot([x.min(), x.max()], [x.min()*a_hat + b_hat, x.max()*a_hat + b_hat], label='RANSAC', c='r')\n",
    "    plt.plot([x.min(), x.max()], [x.min()*lr_a_hat + lr_b_hat, x.max()*lr_a_hat + lr_b_hat], label='Linear regression', c='g')\n",
    "    plt.ylim(-12, 12)\n",
    "    plt.legend()\n",
    "    plt.title(f'a\\u0302={a_hat:.5} b\\u0302={b_hat:.5} r2={r2:.5} mse={mse:.5} mae={mae:.5}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huber Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651ae47e52f34b0fb9aed12f10a6aa84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-2.0, description='a', max=2.0, min=-2.0), FloatSlider(value=-1.0, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(a=FloatSlider(min=-2, max=2, step=0.1, value=-2),\n",
    "          b=FloatSlider(min=-1, max=1, step=0.1, value=-1),\n",
    "          e=FloatSlider(min=0, max=2, step=0.01, value=0.1),\n",
    "          n=IntSlider(min=2, max=200, step=2, value=30),\n",
    "          ox=FloatSlider(min=0, max=5, step=0.1, value=2.5),\n",
    "          oy=FloatSlider(min=-2, max=2, step=0.1, value=0.5),\n",
    "          on=IntSlider(min=0, max=20, step=1, value=5))\n",
    "def linear_regression_1d_ransac(a, b, e=0.5, n=100, ox=0.5, oy=0.5, on=5):\n",
    "    \"\"\"\n",
    "    Сравнение RANSAC и HuberRegressor на одномерной линейной регрессии: y = a*x + b\n",
    "    e : уровень шума\n",
    "    n : количество точек\n",
    "    ox, oy : координаты центра выборосов\n",
    "    on : количество точек с выбросом\n",
    "    \"\"\"\n",
    "    xmin = 0\n",
    "    xmax = 5\n",
    "    np.random.seed(42)\n",
    "    x = np.random.rand(n)*(xmax - xmin) + xmin\n",
    "    y = a*x + b + e*np.random.randn(n)\n",
    "    xo = np.random.randn(on) + ox\n",
    "    yo = np.random.randn(on) + oy\n",
    "    X = np.hstack((x, xo)).reshape((n + on, 1))\n",
    "    Y = np.hstack((y, yo))\n",
    "    \n",
    "    ransac_regressor = RANSACRegressor()\n",
    "    ransac_regressor.fit(X,Y)\n",
    "    a_hat = ransac_regressor.estimator_.coef_[0]\n",
    "    b_hat = ransac_regressor.estimator_.intercept_\n",
    "    \n",
    "    linear_regression = LinearRegression()\n",
    "    linear_regression.fit(X,Y)\n",
    "    lr_a_hat = linear_regression.coef_[0]\n",
    "    lr_b_hat = linear_regression.intercept_\n",
    "    \n",
    "    #\n",
    "    huber = HuberRegressor().fit(X,Y)\n",
    "    lr_a_hu = huber.coef_[0]\n",
    "    lr_b_hu = huber.intercept_\n",
    "    #\n",
    "    \n",
    "    y_pred = a_hat*x + b_hat\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "\n",
    "    \n",
    "    plt.scatter(x, y, c='b', marker='o')\n",
    "    plt.scatter(xo, yo, c='r', marker='^')\n",
    "    plt.plot([x.min(), x.max()], [x.min()*a + b, x.max()*a + b], label='true', c='b')\n",
    "    plt.plot([x.min(), x.max()], [x.min()*a_hat + b_hat, x.max()*a_hat + b_hat], label='RANSAC', c='r')\n",
    "    plt.plot([x.min(), x.max()], [x.min()*lr_a_hat + lr_b_hat, x.max()*lr_a_hat + lr_b_hat], label='Linear regression', c='g')\n",
    "    \n",
    "    plt.plot([x.min(), x.max()], [x.min()*lr_a_hu + lr_b_hu, x.max()*lr_a_hu + lr_b_hu], label='Huber regression', c='c')\n",
    "    \n",
    "    plt.ylim(-12, 12)\n",
    "    plt.legend()\n",
    "    plt.title(f'a\\u0302={a_hat:.5} b\\u0302={b_hat:.5} r2={r2:.5} mse={mse:.5} mae={mae:.5}' )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various Gradient Desent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(lst, size):\n",
    "    for i in range(0, len(lst), size):\n",
    "        yield lst[i:i + size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(10, 20)\n",
      "range(20, 30)\n"
     ]
    }
   ],
   "source": [
    "b = batch(range(10, 75), 10)\n",
    "\n",
    "print (next(b))\n",
    "print (next(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145f0338fa9b4c168e45f46f7e9d6966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-2.0, description='a', max=2.0, min=-2.0), FloatSlider(value=-1.0, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(a=FloatSlider(min=-2, max=2, step=0.1, value=-2),\n",
    "          b=FloatSlider(min=-1, max=1, step=0.1, value=-1),\n",
    "          e=FloatSlider(min=0, max=2, step=0.01, value=0.1),\n",
    "          n=IntSlider(min=2, max=200, step=2, value=30),\n",
    "          ox=FloatSlider(min=0, max=5, step=0.1, value=2.5),\n",
    "          oy=FloatSlider(min=-2, max=2, step=0.1, value=0.5),\n",
    "          on=IntSlider(min=0, max=20, step=1, value=5),\n",
    "          iters=IntSlider(min=1, max=200, step=1, value=1),\n",
    "          alg = (['GD','SGD', 'MinibatchSGD', \"MeanSGD\"]),\n",
    "          lr_type = (['adaptive', \"invscaling\"]))\n",
    "            \n",
    "def linear_regression_1d_different_GD(a, b, e=0.5, n=100, ox=0.5, oy=0.5, on=5, iters = 1, alg = \"MeanSGD\", lr_type = 'adaptive'):\n",
    "    \"\"\"\n",
    "    Оцениваем различные варианты градиентного спукска на одномерной линейной регрессии: y = a*x + b\n",
    "    e : уровень шума\n",
    "    n : количество точек\n",
    "    ox, oy : координаты центра выборосов\n",
    "    on : количество точек с выбросом\n",
    "    iters : количество итераций\n",
    "    \"\"\"\n",
    "    print (alg)\n",
    "    \n",
    "    xmin = 0\n",
    "    xmax = 5\n",
    "    np.random.seed(42)\n",
    "    x = np.random.rand(n)*(xmax - xmin) + xmin\n",
    "    y = a*x + b + e*np.random.randn(n)\n",
    "    xo = np.random.randn(on) + ox\n",
    "    yo = np.random.randn(on) + oy\n",
    "    X = np.hstack((x, xo)).reshape((n + on, 1))\n",
    "    Y = np.hstack((y, yo))\n",
    "\n",
    "    linear_regression = LinearRegression()\n",
    "    linear_regression.fit(X,Y)\n",
    "    lr_a_hat = linear_regression.coef_[0]\n",
    "    lr_b_hat = linear_regression.intercept_\n",
    "    \n",
    "    #\n",
    "    if alg == \"SGD\":\n",
    "        sgd = SGDRegressor(alpha=0.1,learning_rate=lr_type,max_iter=iters,penalty='none')\n",
    "        sgd.fit(X,Y)\n",
    "        lr_a = sgd.coef_[0]\n",
    "        lr_b = sgd.intercept_\n",
    "        y_pred = lr_a*X + lr_b\n",
    "        lr_b = lr_b[0]\n",
    "        mse = mean_squared_error(Y, y_pred)\n",
    "        mae = mean_absolute_error(Y, y_pred)\n",
    "    if alg == \"GD\":\n",
    "        lr_a, lr_b = 0, 0\n",
    "        n_els = len(X)\n",
    "        lr = 0.01  # learning Rate\n",
    "        for i in range(iters): \n",
    "            Y_pred = lr_a*X[1] + lr_b  # The current predicted value of Y\n",
    "            D_m = (-2/n_els) * sum(X[1] * (Y[1] - Y_pred))  # Derivative wrt a\n",
    "            D_c = (-2/n_els) * sum(Y[1] - Y_pred)  # Derivative wrt b\n",
    "            lr_a = lr_a - lr * D_m  # Update m\n",
    "            lr_b = lr_b - lr * D_c  # Update c\n",
    "            y_pred = lr_a*X[1] + lr_b\n",
    "            mse = (Y[1]*Y[1] - y_pred[0]*y_pred[0])**0.5\n",
    "            mae = abs(Y[1] - y_pred[0])\n",
    "    \n",
    "    if alg == \"MinibatchSGD\": #Increment / online fit \n",
    "        n_els = len(X)\n",
    "        batch_size = 5\n",
    "        \n",
    "        X_b = batch(X, batch_size)\n",
    "        Y_b = batch(Y, batch_size)\n",
    "        \n",
    "        sgd = SGDRegressor(alpha=0.1,learning_rate=lr_type,max_iter=iters,penalty='none')\n",
    "        X_first_iter = next (X_b)\n",
    "        Y_first_iter = next (Y_b)\n",
    "        sgd.fit(X_first_iter,Y_first_iter)\n",
    "        \n",
    "        lr_a = sgd.coef_[0]\n",
    "        lr_b = sgd.intercept_\n",
    "        y_pred = lr_a*X_first_iter + lr_b\n",
    "        lr_b = lr_b[0]\n",
    "        mse = mean_squared_error(Y_first_iter, y_pred)\n",
    "        mae = mean_absolute_error(Y_first_iter, y_pred)\n",
    "        for i in range(iters):\n",
    "            if (n_els - (i+1)* batch_size  > 0):\n",
    "                X_iter = next (X_b)\n",
    "                Y_iter = next (Y_b)\n",
    "            else:\n",
    "                X_iter = X[:-batch_size]\n",
    "                Y_iter = Y[:-batch_size]\n",
    "                \n",
    "            sgd.partial_fit(X_iter,Y_iter)\n",
    "\n",
    "            lr_a = sgd.coef_[0]\n",
    "            lr_b = sgd.intercept_\n",
    "            y_pred = lr_a*X + lr_b\n",
    "            lr_b = lr_b[0]    \n",
    "            mse = mean_squared_error(Y, y_pred)\n",
    "            mae = mean_absolute_error(Y, y_pred)\n",
    "\n",
    "    \"\"\"\n",
    "    Усреднённый стохастический градиентный спуск, разработанный независимо Руппертом и Поляком в конце 1980-х \n",
    "    годов, является обычным стохастическим градиентным спуском, который записывает среднее вектора параметров\n",
    "    \"\"\"\n",
    "    if alg == \"MeanSGD\":    \n",
    "    \n",
    "        sgd = SGDRegressor(alpha=0.1,learning_rate=lr_type,max_iter=iters,penalty='none', average=True)\n",
    "        sgd.fit(X,Y)\n",
    "        lr_a = sgd.coef_[0]\n",
    "        lr_b = sgd.intercept_\n",
    "        y_pred = lr_a*X + lr_b\n",
    "        lr_b = lr_b[0]\n",
    "        mse = mean_squared_error(Y, y_pred)\n",
    "        mae = mean_absolute_error(Y, y_pred)\n",
    "    \n",
    "    #\n",
    "\n",
    "    \n",
    "    plt.scatter(x, y, c='b', marker='o')\n",
    "    plt.scatter(xo, yo, c='r', marker='^')\n",
    "    plt.plot([x.min(), x.max()], [x.min()*a + b, x.max()*a + b], label='true', c='b')\n",
    "    plt.plot([x.min(), x.max()], [x.min()*lr_a_hat + lr_b_hat, x.max()*lr_a_hat + lr_b_hat], label='Linear regression', c='g')\n",
    "    \n",
    "    plt.plot([x.min(), x.max()], [x.min()*lr_a + lr_b, x.max()*lr_a + lr_b], label=alg, c='c')\n",
    "    \n",
    "    \n",
    "    plt.ylim(-12, 12)\n",
    "    plt.legend()\n",
    "    #print (lr_a)\n",
    "    #print (lr_b)\n",
    "    #plt.title(f'a\\u0302={lr_a:.5} b\\u0302={lr_b:.5}')\n",
    "    plt.title(f'a\\u0302={lr_a:.5} b\\u0302={lr_b:.5} mse={mse:.5} mae={mae:.5}' )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
